{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model import * \n",
    "import configparser\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgParse = configparser.ConfigParser()\n",
    "cfgParse.read(\"model_train.cfg\")\n",
    "reddit_data_loc = str(cfgParse.get(\"input\", \"train_data\"))\n",
    "model_loc_folder = str(cfgParse.get(\"model\", \"folder\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Multi_Naive_Bayes_Clf_no_embedding = ClfReddit(data_loc=reddit_data_loc,label_col='labels', empty_post_str_indicator='[deleted]', annotator_col='annotators',\n",
    "                 test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_extraction_pipeline__text_feat_ext__stem__do_stem': (True, False), 'clf__alpha': (0, 0.5, 1), 'clf__fit_prior': (True, False)}\n",
      "Grid search for optimal hyper parameters\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   2 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=24)]: Done  60 out of  60 | elapsed:  7.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ColumnTransformer]  (1 of 2) Processing char_feat_extraction, total=   0.2s\n",
      "[ColumnTransformer] .... (2 of 2) Processing word_2_vec, total= 1.4min\n",
      "[ColumnTransformer] . (1 of 2) Processing text_feat_ext, total= 2.5min\n",
      "[ColumnTransformer] ....... (1 of 1) Processing one_hot, total=   0.0s\n",
      "[ColumnTransformer] .. (2 of 2) Processing cat_feat_ext, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgxuser_layersvanguard/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "Multi_Naive_Bayes_Clf_no_embedding.fit(evaluation_metric = 'f1_weighted', hyperparameter_dict={'do_stem':(True,False),'alpha':(0,0.5,1),'fit_prior':(True, False)}, complete_search=True, n_cpus=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([181.42869134,  91.06866336, 182.24180994,  91.52620378,\n",
      "       184.79667082, 108.1904573 , 168.32999959,  94.63106556,\n",
      "       156.24160857,  96.08851104, 141.02558722,  78.05967455]), 'std_fit_time': array([ 3.23356795,  3.28912388,  2.53042281,  4.72553271,  9.21870036,\n",
      "        5.96438571,  7.42789077,  2.55218986,  3.97213407,  2.71043485,\n",
      "       10.94991409,  1.56745305]), 'mean_score_time': array([21.06509542,  1.49834862, 21.32491746,  1.6084034 , 20.07740417,\n",
      "        1.10999765, 19.45281711,  1.40837731, 14.87313352,  1.19208617,\n",
      "       14.79725003,  1.01973634]), 'std_score_time': array([2.01064141, 0.31586588, 0.96242939, 0.37729333, 2.07273513,\n",
      "       0.06405412, 3.12516411, 0.33611579, 0.72378042, 0.11275408,\n",
      "       0.76106309, 0.03001145]), 'param_clf__alpha': masked_array(data=[0, 0, 0, 0, 0.5, 0.5, 0.5, 0.5, 1, 1, 1, 1],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_clf__fit_prior': masked_array(data=[True, True, False, False, True, True, False, False,\n",
      "                   True, True, False, False],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_feature_extraction_pipeline__text_feat_ext__stem__do_stem': masked_array(data=[True, False, True, False, True, False, True, False,\n",
      "                   True, False, True, False],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'clf__alpha': 0, 'clf__fit_prior': True, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': True}, {'clf__alpha': 0, 'clf__fit_prior': True, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': False}, {'clf__alpha': 0, 'clf__fit_prior': False, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': True}, {'clf__alpha': 0, 'clf__fit_prior': False, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': False}, {'clf__alpha': 0.5, 'clf__fit_prior': True, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': True}, {'clf__alpha': 0.5, 'clf__fit_prior': True, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': False}, {'clf__alpha': 0.5, 'clf__fit_prior': False, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': True}, {'clf__alpha': 0.5, 'clf__fit_prior': False, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': False}, {'clf__alpha': 1, 'clf__fit_prior': True, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': True}, {'clf__alpha': 1, 'clf__fit_prior': True, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': False}, {'clf__alpha': 1, 'clf__fit_prior': False, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': True}, {'clf__alpha': 1, 'clf__fit_prior': False, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': False}], 'split0_test_score': array([0.48096511, 0.47868872, 0.45452561, 0.44561507, 0.46352583,\n",
      "       0.48045739, 0.53143662, 0.52689908, 0.40941851, 0.44304174,\n",
      "       0.50214009, 0.50652236]), 'split1_test_score': array([0.46236086, 0.46102017, 0.45048454, 0.44513763, 0.46092867,\n",
      "       0.471532  , 0.51287061, 0.50942995, 0.41610706, 0.44085606,\n",
      "       0.4905233 , 0.49139972]), 'split2_test_score': array([0.46802698, 0.46479763, 0.45668644, 0.44880439, 0.47040703,\n",
      "       0.48076163, 0.52276158, 0.52056709, 0.42803597, 0.45135064,\n",
      "       0.50046947, 0.50485097]), 'split3_test_score': array([0.47298199, 0.46753477, 0.45774269, 0.45117612, 0.46529137,\n",
      "       0.47801182, 0.52166385, 0.51816441, 0.42100662, 0.44375742,\n",
      "       0.49612484, 0.49992344]), 'split4_test_score': array([0.4703718 , 0.46531087, 0.45415311, 0.44805427, 0.46008909,\n",
      "       0.47298188, 0.51506802, 0.51111972, 0.41561486, 0.44131804,\n",
      "       0.49293051, 0.49481106]), 'mean_test_score': array([0.47094147, 0.46747072, 0.45471835, 0.44775731, 0.46404839,\n",
      "       0.47674903, 0.52076045, 0.51723634, 0.41803629, 0.44406474,\n",
      "       0.49643778, 0.49950166]), 'std_test_score': array([0.00611727, 0.00598816, 0.00250212, 0.00220569, 0.00367849,\n",
      "       0.00381729, 0.00653411, 0.00638264, 0.00620766, 0.00379642,\n",
      "       0.00438536, 0.00575737]), 'rank_test_score': array([ 6,  7,  9, 10,  8,  5,  1,  2, 12, 11,  4,  3], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "print(Multi_Naive_Bayes_Clf_no_embedding.cross_val_result_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5207604468025518\n"
     ]
    }
   ],
   "source": [
    "print(Multi_Naive_Bayes_Clf_no_embedding.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random_Forest_Clf_no_embedding = ClfReddit(data_loc=reddit_data_loc, empty_post_str_indicator='[deleted]', label_col='labels', annotator_col='annotators',\n",
    "                 test_size=0.2, model=RandomForestClassifier(class_weight=\"balanced\", verbose=True, random_state=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_extraction_pipeline__text_feat_ext__stem__do_stem': (True, False), 'clf__max_depth': (2, 4, 8, 12)}\n",
      "Grid search for optimal hyper parameters\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done  34 out of  40 | elapsed:  4.4min remaining:   46.4s\n",
      "[Parallel(n_jobs=24)]: Done  40 out of  40 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ColumnTransformer]  (1 of 2) Processing char_feat_extraction, total=   0.2s\n",
      "[ColumnTransformer] .... (2 of 2) Processing word_2_vec, total= 1.4min\n",
      "[ColumnTransformer] . (1 of 2) Processing text_feat_ext, total= 2.5min\n",
      "[ColumnTransformer] ....... (1 of 1) Processing one_hot, total=   0.0s\n",
      "[ColumnTransformer] .. (2 of 2) Processing cat_feat_ext, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgxuser_layersvanguard/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/dgxuser_layersvanguard/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "Random_Forest_Clf_no_embedding.fit(evaluation_metric = 'f1_weighted', hyperparameter_dict={'do_stem':(True,False),'max_depth':(2,4,8,12)}, complete_search=True, n_cpus=24) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([176.25876069,  85.49362712, 176.23445024,  89.05528011,\n",
      "       174.76107292, 106.47247953, 142.42517519,  72.08855333]), 'std_fit_time': array([12.17511485,  2.89048299,  2.4026042 ,  4.91977984, 11.49598894,\n",
      "        3.95445513,  9.6111943 ,  1.30185672]), 'mean_score_time': array([19.50387306,  1.38848553, 18.84370322,  1.67111707, 18.14735832,\n",
      "        1.16804285, 14.84552078,  1.07758541]), 'std_score_time': array([2.75875495, 0.22442684, 2.8123459 , 0.17825632, 2.89257083,\n",
      "       0.15942417, 0.68969399, 0.03588604]), 'param_clf__max_depth': masked_array(data=[2, 2, 4, 4, 8, 8, 12, 12],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_feature_extraction_pipeline__text_feat_ext__stem__do_stem': masked_array(data=[True, False, True, False, True, False, True, False],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'clf__max_depth': 2, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': True}, {'clf__max_depth': 2, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': False}, {'clf__max_depth': 4, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': True}, {'clf__max_depth': 4, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': False}, {'clf__max_depth': 8, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': True}, {'clf__max_depth': 8, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': False}, {'clf__max_depth': 12, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': True}, {'clf__max_depth': 12, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': False}], 'split0_test_score': array([0.07540909, 0.03768125, 0.31108678, 0.10027239, 0.32813453,\n",
      "       0.31971111, 0.2663433 , 0.24248601]), 'split1_test_score': array([0.04848545, 0.06113945, 0.10441853, 0.12459617, 0.13196682,\n",
      "       0.15813734, 0.22429715, 0.33393543]), 'split2_test_score': array([0.07585372, 0.04104565, 0.11223714, 0.06752146, 0.17299291,\n",
      "       0.28342325, 0.29490663, 0.32263632]), 'split3_test_score': array([0.01025115, 0.01309056, 0.1865729 , 0.03922736, 0.23529537,\n",
      "       0.09032251, 0.41155927, 0.24017197]), 'split4_test_score': array([0.04980145, 0.02389248, 0.16142809, 0.10713097, 0.19850988,\n",
      "       0.10352119, 0.22947657, 0.14186246]), 'mean_test_score': array([0.05196133, 0.03537114, 0.17515188, 0.087751  , 0.213382  ,\n",
      "       0.19102996, 0.28531466, 0.25622318]), 'std_test_score': array([0.02398781, 0.01630792, 0.07451404, 0.03050784, 0.06653089,\n",
      "       0.09378432, 0.0681608 , 0.06924316]), 'rank_test_score': array([7, 8, 5, 6, 3, 4, 1, 2], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "print(Random_Forest_Clf_no_embedding.cross_val_result_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28531466151953105\n"
     ]
    }
   ],
   "source": [
    "print(Random_Forest_Clf_no_embedding.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_Clf_no_embedding= ClfReddit(data_loc=reddit_data_loc, do_kernel_transform=True, empty_post_str_indicator='[deleted]', \n",
    "                                label_col='labels', annotator_col='annotators', test_size=0.2, \n",
    "                                self.model = SGDClassifier(loss='hinge', penalty='elasticnet', verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': (True, False), 'feature_extraction_pipeline__kernel_transform__kernel': ('linear', 'rbf', 'poly'), 'feature_extraction_pipeline__kernel_transform__gamma': (0.001, 0.01, 0.1), 'clf__alpha': (0.01, 0.1, 0.5, 1, 2, 5), 'clf__l1_ratio': [0.0, 0.2, 0.4, 0.6000000000000001, 0.8, 1.0]}\n",
      "Randomized search for optimal hyper parameters\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "/home/dgxuser_layersvanguard/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 26.1min\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed: 125.2min\n",
      "[Parallel(n_jobs=4)]: Done 250 out of 250 | elapsed: 161.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ColumnTransformer]  (1 of 2) Processing char_feat_extraction, total=   0.2s\n",
      "[ColumnTransformer] .... (2 of 2) Processing word_2_vec, total= 1.4min\n",
      "[ColumnTransformer] ....... (1 of 1) Processing one_hot, total=   0.0s\n",
      "[Pipeline] .......... (step 1 of 2) Processing feat_ext, total= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgxuser_layersvanguard/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .. (step 2 of 2) Processing kernel_transform, total=   2.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.05, NNZs: 1000, Bias: -1.003442, T: 113223, Avg. loss: 0.101801\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.03, NNZs: 1000, Bias: -1.002624, T: 226446, Avg. loss: 0.100983\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.03, NNZs: 1000, Bias: -1.001463, T: 339669, Avg. loss: 0.100949\n",
      "Total training time: 2.32 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.02, NNZs: 1000, Bias: -1.000992, T: 452892, Avg. loss: 0.100966\n",
      "Total training time: 3.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.02, NNZs: 1000, Bias: -1.001020, T: 566115, Avg. loss: 0.100932\n",
      "Total training time: 3.80 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.02, NNZs: 1000, Bias: -1.000859, T: 679338, Avg. loss: 0.100943\n",
      "Total training time: 4.58 seconds.\n",
      "Convergence after 6 epochs took 4.58 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.23, NNZs: 1000, Bias: -1.222465, T: 113223, Avg. loss: 0.028266\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.20, NNZs: 1000, Bias: -1.158417, T: 226446, Avg. loss: 0.027014\n",
      "Total training time: 1.36 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.14, NNZs: 1000, Bias: -1.129267, T: 339669, Avg. loss: 0.027046\n",
      "Total training time: 2.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.12, NNZs: 1000, Bias: -1.113161, T: 452892, Avg. loss: 0.026902\n",
      "Total training time: 2.60 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.11, NNZs: 1000, Bias: -1.102951, T: 566115, Avg. loss: 0.026865\n",
      "Total training time: 3.13 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.10, NNZs: 1000, Bias: -1.093013, T: 679338, Avg. loss: 0.026865\n",
      "Total training time: 3.69 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.09, NNZs: 1000, Bias: -1.086112, T: 792561, Avg. loss: 0.026797\n",
      "Total training time: 4.35 seconds.\n",
      "Convergence after 7 epochs took 4.35 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.92, NNZs: 1000, Bias: -0.810065, T: 113223, Avg. loss: 0.499004\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.93, NNZs: 1000, Bias: -0.811421, T: 226446, Avg. loss: 0.494247\n",
      "Total training time: 1.31 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.93, NNZs: 1000, Bias: -0.809032, T: 339669, Avg. loss: 0.493894\n",
      "Total training time: 1.93 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.93, NNZs: 1000, Bias: -0.809947, T: 452892, Avg. loss: 0.493802\n",
      "Total training time: 2.54 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.93, NNZs: 1000, Bias: -0.809996, T: 566115, Avg. loss: 0.493751\n",
      "Total training time: 3.15 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.93, NNZs: 1000, Bias: -0.810018, T: 679338, Avg. loss: 0.493801\n",
      "Total training time: 3.87 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.93, NNZs: 1000, Bias: -0.810441, T: 792561, Avg. loss: 0.493730\n",
      "Total training time: 4.48 seconds.\n",
      "Convergence after 7 epochs took 4.48 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.85, NNZs: 1000, Bias: -1.012471, T: 113223, Avg. loss: 0.183554\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.83, NNZs: 1000, Bias: -1.013428, T: 226446, Avg. loss: 0.180321\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.83, NNZs: 1000, Bias: -1.012463, T: 339669, Avg. loss: 0.180190\n",
      "Total training time: 2.35 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.83, NNZs: 1000, Bias: -1.012449, T: 452892, Avg. loss: 0.180174\n",
      "Total training time: 3.10 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.83, NNZs: 1000, Bias: -1.012699, T: 566115, Avg. loss: 0.180154\n",
      "Total training time: 3.75 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.82, NNZs: 1000, Bias: -1.012792, T: 679338, Avg. loss: 0.180129\n",
      "Total training time: 4.46 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.82, NNZs: 1000, Bias: -1.011692, T: 792561, Avg. loss: 0.180123\n",
      "Total training time: 5.14 seconds.\n",
      "Convergence after 7 epochs took 5.14 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.03, NNZs: 1000, Bias: -1.002246, T: 113223, Avg. loss: 0.074407\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.02, NNZs: 1000, Bias: -1.002012, T: 226446, Avg. loss: 0.073796\n",
      "Total training time: 1.41 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.001595, T: 339669, Avg. loss: 0.073769\n",
      "Total training time: 2.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.001272, T: 452892, Avg. loss: 0.073765\n",
      "Total training time: 2.92 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.000857, T: 566115, Avg. loss: 0.073777\n",
      "Total training time: 3.51 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.000846, T: 679338, Avg. loss: 0.073747\n",
      "Total training time: 4.17 seconds.\n",
      "Convergence after 6 epochs took 4.17 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 1000, Bias: -1.002714, T: 113223, Avg. loss: 0.396314\n",
      "Total training time: 0.86 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.05, NNZs: 1000, Bias: -1.002323, T: 226446, Avg. loss: 0.392848\n",
      "Total training time: 1.63 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.04, NNZs: 1000, Bias: -1.002573, T: 339669, Avg. loss: 0.392710\n",
      "Total training time: 2.37 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.03, NNZs: 1000, Bias: -1.001490, T: 452892, Avg. loss: 0.392640\n",
      "Total training time: 3.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.03, NNZs: 1000, Bias: -1.001302, T: 566115, Avg. loss: 0.392579\n",
      "Total training time: 3.66 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.02, NNZs: 1000, Bias: -1.001293, T: 679338, Avg. loss: 0.392546\n",
      "Total training time: 4.39 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.02, NNZs: 1000, Bias: -1.001150, T: 792561, Avg. loss: 0.392537\n",
      "Total training time: 5.11 seconds.\n",
      "Convergence after 7 epochs took 5.11 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.03, NNZs: 1000, Bias: -1.001222, T: 113223, Avg. loss: 0.063815\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.02, NNZs: 1000, Bias: -1.001287, T: 226446, Avg. loss: 0.063155\n",
      "Total training time: 1.41 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.02, NNZs: 1000, Bias: -1.000824, T: 339669, Avg. loss: 0.063275\n",
      "Total training time: 2.11 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.000527, T: 452892, Avg. loss: 0.063109\n",
      "Total training time: 2.75 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.000502, T: 566115, Avg. loss: 0.063104\n",
      "Total training time: 3.42 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.000323, T: 679338, Avg. loss: 0.063116\n",
      "Total training time: 4.09 seconds.\n",
      "Convergence after 6 epochs took 4.09 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.03, NNZs: 1000, Bias: -1.003759, T: 113223, Avg. loss: 0.048178\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.02, NNZs: 1000, Bias: -1.001883, T: 226446, Avg. loss: 0.047642\n",
      "Total training time: 1.22 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.02, NNZs: 1000, Bias: -1.000988, T: 339669, Avg. loss: 0.047617\n",
      "Total training time: 1.78 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.001300, T: 452892, Avg. loss: 0.047610\n",
      "Total training time: 2.37 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.000904, T: 566115, Avg. loss: 0.047605\n",
      "Total training time: 2.97 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.001358, T: 679338, Avg. loss: 0.047602\n",
      "Total training time: 3.58 seconds.\n",
      "Convergence after 6 epochs took 3.58 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.03, NNZs: 1000, Bias: -1.003246, T: 113223, Avg. loss: 0.056721\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.06, NNZs: 1000, Bias: -1.001564, T: 226446, Avg. loss: 0.055728\n",
      "Total training time: 1.18 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.000952, T: 339669, Avg. loss: 0.055690\n",
      "Total training time: 1.81 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.000540, T: 452892, Avg. loss: 0.055640\n",
      "Total training time: 2.40 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.000596, T: 566115, Avg. loss: 0.055622\n",
      "Total training time: 2.97 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.000459, T: 679338, Avg. loss: 0.055614\n",
      "Total training time: 3.56 seconds.\n",
      "Convergence after 6 epochs took 3.56 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.80, NNZs: 1000, Bias: -0.835317, T: 113223, Avg. loss: 0.232288\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.80, NNZs: 1000, Bias: -0.834423, T: 226446, Avg. loss: 0.229320\n",
      "Total training time: 1.21 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.81, NNZs: 1000, Bias: -0.832350, T: 339669, Avg. loss: 0.229095\n",
      "Total training time: 1.81 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.81, NNZs: 1000, Bias: -0.832159, T: 452892, Avg. loss: 0.228948\n",
      "Total training time: 2.44 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.82, NNZs: 1000, Bias: -0.830981, T: 566115, Avg. loss: 0.228798\n",
      "Total training time: 3.10 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.82, NNZs: 1000, Bias: -0.829989, T: 679338, Avg. loss: 0.228838\n",
      "Total training time: 3.77 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.82, NNZs: 1000, Bias: -0.830309, T: 792561, Avg. loss: 0.228739\n",
      "Total training time: 4.48 seconds.\n",
      "Convergence after 7 epochs took 4.48 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   43.6s finished\n"
     ]
    }
   ],
   "source": [
    "SVM_Clf_no_embedding.fit(evaluation_metric='f1_weighted', hyperparameter_dict={'do_stem': (True, False),\n",
    "                    'kernel': ('linear','rbf', 'poly'), \n",
    "                   'gamma':(0.001, 0.01, 0.1), \n",
    "                   'alpha': (0.01, 0.1, 0.5, 1, 2, 5), \n",
    "                   'l1_ratio': list(np.arange(0, 1.1, 0.2))}, complete_search=False,  n_cpus = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([149.6837543 ,  97.91262403, 109.74540877, 153.87997417,\n",
      "       154.64908452, 101.47630897, 157.52105026, 161.53327513,\n",
      "       101.70708561,  99.92568984, 140.06196136, 108.8071888 ,\n",
      "       150.62221856, 149.47222114, 151.37142377, 104.05207796,\n",
      "       170.54351878, 159.42016611, 160.76677151, 106.47367468,\n",
      "       154.50991292, 160.63251576, 105.80091162, 102.11083841,\n",
      "       165.12122097, 122.14822998, 289.13696356, 108.00592132,\n",
      "       160.80303998,  99.55583143, 633.28883166, 152.11646729,\n",
      "       122.13350773, 155.94375267, 107.83237247, 100.86476665,\n",
      "       105.05233655, 102.01099782, 104.05435572, 162.67769938,\n",
      "       101.84768014, 154.28587976, 159.45535946, 155.76151853,\n",
      "       103.07058549, 106.00607972, 181.51268687, 152.61447244,\n",
      "       155.30035596,  99.92988667]), 'std_fit_time': array([  0.97316896,   1.60347737,   9.46069002,   1.62136666,\n",
      "         1.38001044,   1.40386329,   5.55586468,   1.87782737,\n",
      "         1.31124358,   0.31214778,  46.6217815 ,   3.53402772,\n",
      "         0.9568284 ,  10.74529551,   1.90220688,   3.27100883,\n",
      "        32.70535218,   1.81176499,  12.51489208,   3.88770278,\n",
      "         2.1592435 ,   1.79978636,   1.58524732,   1.96776734,\n",
      "        12.38305945,   8.54421164, 162.45386074,   5.68725174,\n",
      "         2.04373585,   2.10467829, 258.10722653,   1.48727942,\n",
      "        32.75020687,   2.34051697,   1.50177423,   0.88915187,\n",
      "         4.11497602,   8.28459123,   1.16311863,   3.10451761,\n",
      "         0.92945359,   8.54712928,   1.64355459,   1.97112202,\n",
      "         1.69005234,   4.96200164,  25.46770373,   3.03134144,\n",
      "         1.99313204,   1.28266094]), 'mean_score_time': array([16.22537065,  2.36584196,  2.19339991, 15.75496869, 15.70248556,\n",
      "        2.92788367, 16.24678626, 17.26036062,  1.64806228,  2.19391074,\n",
      "        2.89562283,  2.24574685, 16.20538592,  2.87899518, 16.4114984 ,\n",
      "        1.69338999,  2.8605969 , 16.35736465, 15.90436482,  2.23159876,\n",
      "       16.12532172, 16.50535207,  1.65410767,  1.78674898, 17.23719869,\n",
      "        2.87745299,  2.81886659,  2.87809844, 17.08530855,  2.31549683,\n",
      "        2.8635148 , 16.91441121,  1.65695953, 15.69408054,  1.64660077,\n",
      "        2.22244406,  1.65410986,  2.26396594,  1.74724169, 16.06442518,\n",
      "        1.64126811, 16.35625167, 17.10094204, 15.7368978 ,  1.73317285,\n",
      "        2.94810805, 16.89951663, 16.27850642, 15.90291982,  2.18151073]), 'std_score_time': array([0.65353677, 0.29597026, 0.06206848, 0.89853729, 0.76038394,\n",
      "       0.11041229, 0.71168134, 1.11281144, 0.04166644, 0.06794259,\n",
      "       0.08680578, 0.08159013, 0.77984477, 0.04404525, 0.84002249,\n",
      "       0.09545855, 0.03815907, 0.8083404 , 0.65666263, 0.08558037,\n",
      "       0.70675932, 0.41398506, 0.02715624, 0.20350909, 0.50117266,\n",
      "       0.05734397, 0.06718093, 0.06201074, 0.95894584, 0.1298127 ,\n",
      "       0.0798207 , 0.84793653, 0.06306405, 0.82575972, 0.06012673,\n",
      "       0.11752167, 0.03495984, 0.04006556, 0.0983626 , 0.98363001,\n",
      "       0.04707101, 0.84867883, 0.89841009, 0.74072045, 0.15093153,\n",
      "       0.15234036, 0.81252604, 0.77329807, 0.75138517, 0.10657637]), 'param_feature_extraction_pipeline__kernel_transform__kernel': masked_array(data=['rbf', 'rbf', 'rbf', 'linear', 'linear', 'poly', 'rbf',\n",
      "                   'poly', 'linear', 'rbf', 'poly', 'rbf', 'rbf', 'poly',\n",
      "                   'rbf', 'linear', 'poly', 'rbf', 'linear', 'rbf',\n",
      "                   'linear', 'rbf', 'linear', 'linear', 'poly', 'poly',\n",
      "                   'poly', 'poly', 'poly', 'rbf', 'poly', 'poly',\n",
      "                   'linear', 'linear', 'linear', 'rbf', 'linear', 'rbf',\n",
      "                   'linear', 'linear', 'linear', 'rbf', 'poly', 'linear',\n",
      "                   'linear', 'poly', 'poly', 'rbf', 'linear', 'rbf'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_feature_extraction_pipeline__kernel_transform__gamma': masked_array(data=[0.1, 0.01, 0.1, 0.1, 0.1, 0.001, 0.001, 0.01, 0.01,\n",
      "                   0.01, 0.1, 0.1, 0.01, 0.1, 0.1, 0.1, 0.1, 0.01, 0.001,\n",
      "                   0.1, 0.001, 0.01, 0.001, 0.01, 0.1, 0.01, 0.01, 0.01,\n",
      "                   0.01, 0.001, 0.1, 0.01, 0.01, 0.1, 0.001, 0.1, 0.01,\n",
      "                   0.01, 0.001, 0.01, 0.001, 0.01, 0.001, 0.1, 0.001,\n",
      "                   0.01, 0.1, 0.1, 0.001, 0.01],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': masked_array(data=[True, False, False, True, True, False, True, True,\n",
      "                   False, False, False, False, True, False, True, False,\n",
      "                   False, True, True, False, True, True, False, False,\n",
      "                   True, False, False, False, True, False, False, True,\n",
      "                   False, True, False, False, False, False, False, True,\n",
      "                   False, True, True, True, False, False, True, True,\n",
      "                   True, False],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_clf__l1_ratio': masked_array(data=[0.2, 0.4, 1.0, 1.0, 0.2, 0.8, 0.4, 0.0, 0.4, 0.4,\n",
      "                   0.6000000000000001, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0,\n",
      "                   0.8, 0.2, 0.4, 0.0, 0.0, 0.4, 0.4, 0.2, 1.0, 0.2, 0.0,\n",
      "                   0.6000000000000001, 0.8, 0.2, 0.8, 0.2, 0.0, 1.0, 1.0,\n",
      "                   0.2, 0.6000000000000001, 0.0, 0.2, 0.2, 0.0,\n",
      "                   0.6000000000000001, 0.4, 0.4, 0.4, 0.8, 0.2, 0.2],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_clf__alpha': masked_array(data=[0.1, 0.1, 2, 1, 1, 0.5, 5, 0.5, 0.1, 1, 5, 5, 0.5, 2,\n",
      "                   0.01, 0.1, 0.1, 1, 2, 5, 5, 2, 0.5, 0.5, 2, 0.1, 0.5,\n",
      "                   2, 5, 1, 0.1, 0.5, 5, 0.01, 0.1, 0.01, 1, 2, 1, 0.01,\n",
      "                   1, 2, 0.5, 1, 0.1, 2, 0.5, 2, 1, 0.01],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'feature_extraction_pipeline__kernel_transform__kernel': 'rbf', 'feature_extraction_pipeline__kernel_transform__gamma': 0.1, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': True, 'clf__l1_ratio': 0.2, 'clf__alpha': 0.1}, {'feature_extraction_pipeline__kernel_transform__kernel': 'rbf', 'feature_extraction_pipeline__kernel_transform__gamma': 0.01, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 0.4, 'clf__alpha': 0.1}, {'feature_extraction_pipeline__kernel_transform__kernel': 'rbf', 'feature_extraction_pipeline__kernel_transform__gamma': 0.1, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 1.0, 'clf__alpha': 2}, {'feature_extraction_pipeline__kernel_transform__kernel': 'linear', 'feature_extraction_pipeline__kernel_transform__gamma': 0.1, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': True, 'clf__l1_ratio': 1.0, 'clf__alpha': 1}, {'feature_extraction_pipeline__kernel_transform__kernel': 'linear', 'feature_extraction_pipeline__kernel_transform__gamma': 0.1, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': True, 'clf__l1_ratio': 0.2, 'clf__alpha': 1}, {'feature_extraction_pipeline__kernel_transform__kernel': 'poly', 'feature_extraction_pipeline__kernel_transform__gamma': 0.001, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 0.8, 'clf__alpha': 0.5}, {'feature_extraction_pipeline__kernel_transform__kernel': 'rbf', 'feature_extraction_pipeline__kernel_transform__gamma': 0.001, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': True, 'clf__l1_ratio': 0.4, 'clf__alpha': 5}, {'feature_extraction_pipeline__kernel_transform__kernel': 'poly', 'feature_extraction_pipeline__kernel_transform__gamma': 0.01, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': True, 'clf__l1_ratio': 0.0, 'clf__alpha': 0.5}, {'feature_extraction_pipeline__kernel_transform__kernel': 'linear', 'feature_extraction_pipeline__kernel_transform__gamma': 0.01, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 0.4, 'clf__alpha': 0.1}, {'feature_extraction_pipeline__kernel_transform__kernel': 'rbf', 'feature_extraction_pipeline__kernel_transform__gamma': 0.01, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 0.4, 'clf__alpha': 1}, {'feature_extraction_pipeline__kernel_transform__kernel': 'poly', 'feature_extraction_pipeline__kernel_transform__gamma': 0.1, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 0.6000000000000001, 'clf__alpha': 5}, {'feature_extraction_pipeline__kernel_transform__kernel': 'rbf', 'feature_extraction_pipeline__kernel_transform__gamma': 0.1, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 0.0, 'clf__alpha': 5}, {'feature_extraction_pipeline__kernel_transform__kernel': 'rbf', 'feature_extraction_pipeline__kernel_transform__gamma': 0.01, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': True, 'clf__l1_ratio': 1.0, 'clf__alpha': 0.5}, {'feature_extraction_pipeline__kernel_transform__kernel': 'poly', 'feature_extraction_pipeline__kernel_transform__gamma': 0.1, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 0.0, 'clf__alpha': 2}, {'feature_extraction_pipeline__kernel_transform__kernel': 'rbf', 'feature_extraction_pipeline__kernel_transform__gamma': 0.1, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': True, 'clf__l1_ratio': 1.0, 'clf__alpha': 0.01}, {'feature_extraction_pipeline__kernel_transform__kernel': 'linear', 'feature_extraction_pipeline__kernel_transform__gamma': 0.1, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 1.0, 'clf__alpha': 0.1}, {'feature_extraction_pipeline__kernel_transform__kernel': 'poly', 'feature_extraction_pipeline__kernel_transform__gamma': 0.1, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 0.0, 'clf__alpha': 0.1}, {'feature_extraction_pipeline__kernel_transform__kernel': 'rbf', 'feature_extraction_pipeline__kernel_transform__gamma': 0.01, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': True, 'clf__l1_ratio': 0.0, 'clf__alpha': 1}, {'feature_extraction_pipeline__kernel_transform__kernel': 'linear', 'feature_extraction_pipeline__kernel_transform__gamma': 0.001, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': True, 'clf__l1_ratio': 0.8, 'clf__alpha': 2}, {'feature_extraction_pipeline__kernel_transform__kernel': 'rbf', 'feature_extraction_pipeline__kernel_transform__gamma': 0.1, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 0.2, 'clf__alpha': 5}, {'feature_extraction_pipeline__kernel_transform__kernel': 'linear', 'feature_extraction_pipeline__kernel_transform__gamma': 0.001, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': True, 'clf__l1_ratio': 0.4, 'clf__alpha': 5}, {'feature_extraction_pipeline__kernel_transform__kernel': 'rbf', 'feature_extraction_pipeline__kernel_transform__gamma': 0.01, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': True, 'clf__l1_ratio': 0.0, 'clf__alpha': 2}, {'feature_extraction_pipeline__kernel_transform__kernel': 'linear', 'feature_extraction_pipeline__kernel_transform__gamma': 0.001, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 0.0, 'clf__alpha': 0.5}, {'feature_extraction_pipeline__kernel_transform__kernel': 'linear', 'feature_extraction_pipeline__kernel_transform__gamma': 0.01, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 0.4, 'clf__alpha': 0.5}, {'feature_extraction_pipeline__kernel_transform__kernel': 'poly', 'feature_extraction_pipeline__kernel_transform__gamma': 0.1, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': True, 'clf__l1_ratio': 0.4, 'clf__alpha': 2}, {'feature_extraction_pipeline__kernel_transform__kernel': 'poly', 'feature_extraction_pipeline__kernel_transform__gamma': 0.01, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 0.2, 'clf__alpha': 0.1}, {'feature_extraction_pipeline__kernel_transform__kernel': 'poly', 'feature_extraction_pipeline__kernel_transform__gamma': 0.01, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 1.0, 'clf__alpha': 0.5}, {'feature_extraction_pipeline__kernel_transform__kernel': 'poly', 'feature_extraction_pipeline__kernel_transform__gamma': 0.01, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 0.2, 'clf__alpha': 2}, {'feature_extraction_pipeline__kernel_transform__kernel': 'poly', 'feature_extraction_pipeline__kernel_transform__gamma': 0.01, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': True, 'clf__l1_ratio': 0.0, 'clf__alpha': 5}, {'feature_extraction_pipeline__kernel_transform__kernel': 'rbf', 'feature_extraction_pipeline__kernel_transform__gamma': 0.001, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 0.6000000000000001, 'clf__alpha': 1}, {'feature_extraction_pipeline__kernel_transform__kernel': 'poly', 'feature_extraction_pipeline__kernel_transform__gamma': 0.1, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 0.8, 'clf__alpha': 0.1}, {'feature_extraction_pipeline__kernel_transform__kernel': 'poly', 'feature_extraction_pipeline__kernel_transform__gamma': 0.01, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': True, 'clf__l1_ratio': 0.2, 'clf__alpha': 0.5}, {'feature_extraction_pipeline__kernel_transform__kernel': 'linear', 'feature_extraction_pipeline__kernel_transform__gamma': 0.01, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 0.8, 'clf__alpha': 5}, {'feature_extraction_pipeline__kernel_transform__kernel': 'linear', 'feature_extraction_pipeline__kernel_transform__gamma': 0.1, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': True, 'clf__l1_ratio': 0.2, 'clf__alpha': 0.01}, {'feature_extraction_pipeline__kernel_transform__kernel': 'linear', 'feature_extraction_pipeline__kernel_transform__gamma': 0.001, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 0.0, 'clf__alpha': 0.1}, {'feature_extraction_pipeline__kernel_transform__kernel': 'rbf', 'feature_extraction_pipeline__kernel_transform__gamma': 0.1, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 1.0, 'clf__alpha': 0.01}, {'feature_extraction_pipeline__kernel_transform__kernel': 'linear', 'feature_extraction_pipeline__kernel_transform__gamma': 0.01, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 1.0, 'clf__alpha': 1}, {'feature_extraction_pipeline__kernel_transform__kernel': 'rbf', 'feature_extraction_pipeline__kernel_transform__gamma': 0.01, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 0.2, 'clf__alpha': 2}, {'feature_extraction_pipeline__kernel_transform__kernel': 'linear', 'feature_extraction_pipeline__kernel_transform__gamma': 0.001, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 0.6000000000000001, 'clf__alpha': 1}, {'feature_extraction_pipeline__kernel_transform__kernel': 'linear', 'feature_extraction_pipeline__kernel_transform__gamma': 0.01, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': True, 'clf__l1_ratio': 0.0, 'clf__alpha': 0.01}, {'feature_extraction_pipeline__kernel_transform__kernel': 'linear', 'feature_extraction_pipeline__kernel_transform__gamma': 0.001, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 0.2, 'clf__alpha': 1}, {'feature_extraction_pipeline__kernel_transform__kernel': 'rbf', 'feature_extraction_pipeline__kernel_transform__gamma': 0.01, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': True, 'clf__l1_ratio': 0.2, 'clf__alpha': 2}, {'feature_extraction_pipeline__kernel_transform__kernel': 'poly', 'feature_extraction_pipeline__kernel_transform__gamma': 0.001, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': True, 'clf__l1_ratio': 0.0, 'clf__alpha': 0.5}, {'feature_extraction_pipeline__kernel_transform__kernel': 'linear', 'feature_extraction_pipeline__kernel_transform__gamma': 0.1, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': True, 'clf__l1_ratio': 0.6000000000000001, 'clf__alpha': 1}, {'feature_extraction_pipeline__kernel_transform__kernel': 'linear', 'feature_extraction_pipeline__kernel_transform__gamma': 0.001, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 0.4, 'clf__alpha': 0.1}, {'feature_extraction_pipeline__kernel_transform__kernel': 'poly', 'feature_extraction_pipeline__kernel_transform__gamma': 0.01, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 0.4, 'clf__alpha': 2}, {'feature_extraction_pipeline__kernel_transform__kernel': 'poly', 'feature_extraction_pipeline__kernel_transform__gamma': 0.1, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': True, 'clf__l1_ratio': 0.4, 'clf__alpha': 0.5}, {'feature_extraction_pipeline__kernel_transform__kernel': 'rbf', 'feature_extraction_pipeline__kernel_transform__gamma': 0.1, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': True, 'clf__l1_ratio': 0.8, 'clf__alpha': 2}, {'feature_extraction_pipeline__kernel_transform__kernel': 'linear', 'feature_extraction_pipeline__kernel_transform__gamma': 0.001, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': True, 'clf__l1_ratio': 0.2, 'clf__alpha': 1}, {'feature_extraction_pipeline__kernel_transform__kernel': 'rbf', 'feature_extraction_pipeline__kernel_transform__gamma': 0.01, 'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': False, 'clf__l1_ratio': 0.2, 'clf__alpha': 0.01}], 'split0_test_score': array([0.04123891, 0.20197205, 0.20197205, 0.04123891, 0.20197205,\n",
      "       0.20197205, 0.20197205, 0.48016897, 0.20197205, 0.20197205,\n",
      "       0.13074732, 0.46060584, 0.01593765, 0.44184898, 0.20197205,\n",
      "       0.20206657, 0.36595844, 0.46122023, 0.06437351, 0.20197205,\n",
      "       0.20197205, 0.37998992, 0.443929  , 0.06437351, 0.06437351,\n",
      "       0.19886473, 0.2486599 , 0.18913616, 0.42464748, 0.00262192,\n",
      "       0.15879799, 0.20197205, 0.20197205, 0.39542004, 0.43323297,\n",
      "       0.20197205, 0.06437351, 0.20197205, 0.01593765, 0.49248705,\n",
      "       0.06437351, 0.01593765, 0.46295579, 0.01593765, 0.31645154,\n",
      "       0.00484931, 0.20197205, 0.20197205, 0.20197205, 0.30194978]), 'split1_test_score': array([0.015939  , 0.015939  , 0.20198747, 0.0412423 , 0.06437873,\n",
      "       0.00150582, 0.20198747, 0.46958419, 0.00262215, 0.20198747,\n",
      "       0.21870519, 0.39995784, 0.20198747, 0.3255528 , 0.06437873,\n",
      "       0.015939  , 0.28750259, 0.45288936, 0.06437873, 0.20198747,\n",
      "       0.00484973, 0.38731159, 0.40228279, 0.20198747, 0.24751086,\n",
      "       0.37413587, 0.22018097, 0.2251657 , 0.37348721, 0.20198747,\n",
      "       0.24956773, 0.07222556, 0.0412423 , 0.38364857, 0.43026133,\n",
      "       0.0412423 , 0.20376301, 0.0412423 , 0.06437873, 0.435376  ,\n",
      "       0.20198747, 0.20198747, 0.36396345, 0.20198747, 0.20198747,\n",
      "       0.17530353, 0.12136204, 0.20198747, 0.20198747, 0.17443537]), 'split2_test_score': array([0.06436779, 0.0048427 , 0.20203373, 0.0412525 , 0.20203373,\n",
      "       0.06436779, 0.20203373, 0.37764645, 0.24138524, 0.20203373,\n",
      "       0.17692276, 0.39029009, 0.06436779, 0.30884209, 0.01594304,\n",
      "       0.01594304, 0.3344904 , 0.43505834, 0.20203373, 0.20203373,\n",
      "       0.20203373, 0.37268723, 0.41773756, 0.06436779, 0.20499202,\n",
      "       0.18151612, 0.20364863, 0.17891038, 0.39789371, 0.20203373,\n",
      "       0.20302232, 0.20203373, 0.20203373, 0.37556295, 0.41875248,\n",
      "       0.20203373, 0.20203373, 0.20203373, 0.0412525 , 0.4768438 ,\n",
      "       0.01594304, 0.20203373, 0.37667383, 0.0412525 , 0.30002293,\n",
      "       0.20218687, 0.31024121, 0.06436779, 0.20203373, 0.24568912]), 'split3_test_score': array([0.20204916, 0.04123387, 0.04123387, 0.20204916, 0.04123387,\n",
      "       0.01897957, 0.04123387, 0.45593561, 0.26787823, 0.20204916,\n",
      "       0.07981034, 0.40591765, 0.20204916, 0.39339156, 0.20204916,\n",
      "       0.20204916, 0.2665005 , 0.43360393, 0.064373  , 0.20204916,\n",
      "       0.20204916, 0.38257469, 0.43437931, 0.20204916, 0.07362124,\n",
      "       0.2691747 , 0.17574425, 0.19481672, 0.40915245, 0.20204916,\n",
      "       0.26863986, 0.064373  , 0.20204916, 0.38041852, 0.43476635,\n",
      "       0.01594439, 0.20204916, 0.20204916, 0.04123387, 0.42834194,\n",
      "       0.04123387, 0.064373  , 0.44384103, 0.01594439, 0.20719724,\n",
      "       0.10394886, 0.24538187, 0.00192786, 0.20204916, 0.31772451]), 'split4_test_score': array([2.02080013e-01, 6.43834452e-02, 2.02080013e-01, 6.43834452e-02,\n",
      "       2.02080013e-01, 4.84395347e-03, 2.02080013e-01, 3.88148655e-01,\n",
      "       3.71378891e-02, 4.12406665e-02, 1.86835648e-01, 2.02080013e-01,\n",
      "       1.59470893e-02, 1.99164328e-01, 1.92819496e-03, 2.02621998e-01,\n",
      "       3.50082449e-01, 4.82441757e-01, 1.59470893e-02, 6.43834452e-02,\n",
      "       2.02080013e-01, 4.49630609e-01, 4.23865095e-01, 2.02080013e-01,\n",
      "       1.21969325e-01, 1.28090361e-01, 1.02112901e-02, 1.73052030e-01,\n",
      "       4.38566568e-01, 2.02080013e-01, 1.31106043e-01, 3.53468335e-04,\n",
      "       6.43834452e-02, 3.75223898e-01, 4.18186053e-01, 6.43834452e-02,\n",
      "       2.02080013e-01, 1.59470893e-02, 6.43834452e-02, 4.65326312e-01,\n",
      "       4.84395347e-03, 2.02080013e-01, 3.77066507e-01, 1.59470893e-02,\n",
      "       2.02080013e-01, 1.81217239e-01, 2.59606114e-01, 6.43834452e-02,\n",
      "       2.02080013e-01, 4.28557607e-02]), 'mean_test_score': array([0.10512693, 0.06567796, 0.1698637 , 0.07803025, 0.14233903,\n",
      "       0.05833917, 0.1698637 , 0.43430036, 0.15019899, 0.16986173,\n",
      "       0.15860481, 0.37177837, 0.10005889, 0.33376659, 0.09725866,\n",
      "       0.12772098, 0.32090729, 0.4530424 , 0.08222202, 0.17448955,\n",
      "       0.16259275, 0.39443675, 0.42443878, 0.14696818, 0.14249457,\n",
      "       0.23036142, 0.17169725, 0.19221745, 0.40874832, 0.16214847,\n",
      "       0.20222781, 0.10819717, 0.14233711, 0.38205551, 0.42704031,\n",
      "       0.10511878, 0.17485579, 0.13265137, 0.04543623, 0.45967566,\n",
      "       0.06568176, 0.13727873, 0.40490148, 0.05821763, 0.24555068,\n",
      "       0.13349672, 0.22770793, 0.10693565, 0.20202448, 0.21653652]), 'std_test_score': array([8.06108512e-02, 7.12082480e-02, 6.43120848e-02, 6.26509029e-02,\n",
      "       7.34689009e-02, 7.52470195e-02, 6.43120848e-02, 4.27939207e-02,\n",
      "       1.09000003e-01, 6.43041461e-02, 4.84337017e-02, 8.83025568e-02,\n",
      "       8.51078763e-02, 8.25130543e-02, 8.80074149e-02, 9.12712068e-02,\n",
      "       3.77924437e-02, 1.80684269e-02, 6.27721810e-02, 5.50475876e-02,\n",
      "       7.88767457e-02, 2.79968607e-02, 1.42471535e-02, 6.74440849e-02,\n",
      "       7.23928902e-02, 8.48708940e-02, 8.41259396e-02, 1.81454234e-02,\n",
      "       2.23752896e-02, 7.97707647e-02, 5.21461764e-02, 8.05519057e-02,\n",
      "       7.34612559e-02, 7.38472533e-03, 7.14828653e-03, 8.05797423e-02,\n",
      "       5.52502883e-02, 8.53344047e-02, 1.80180712e-02, 2.43954056e-02,\n",
      "       7.12111080e-02, 8.07713313e-02, 4.03349268e-02, 7.25548696e-02,\n",
      "       5.14841137e-02, 7.23446577e-02, 6.34260556e-02, 8.08916741e-02,\n",
      "       3.97411549e-05, 1.00317060e-01]), 'rank_test_score': array([40, 47, 23, 45, 32, 48, 23,  3, 29, 25, 28, 10, 42, 11, 43, 37, 12,\n",
      "        2, 44, 21, 26,  8,  5, 30, 31, 14, 22, 19,  6, 27, 17, 38, 33,  9,\n",
      "        4, 41, 20, 36, 50,  1, 46, 34,  7, 49, 13, 35, 15, 39, 18, 16],\n",
      "      dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "print(SVM_Clf_no_embedding.cross_val_result_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4596756609276255\n"
     ]
    }
   ],
   "source": [
    "print(SVM_Clf_no_embedding.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Multi_Naive_Bayes_Clf = ClfReddit(data_loc = reddit_data_loc, word_2_vec_dim=300,depth_post_col='depth',\n",
    "                 empty_post_str_indicator='[deleted]', label_col='labels', annotator_col='annotators',\n",
    "                 test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_extraction_pipeline__text_feat_ext__stem__do_stem': (True, False), 'clf__alpha': (0, 0.5, 1), 'clf__fit_prior': (True, False)}\n",
      "Grid search for optimal hyper parameters\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   2 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=24)]: Done  60 out of  60 | elapsed: 15.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ColumnTransformer]  (1 of 2) Processing char_feat_extraction, total=   0.2s\n",
      "(300,)\n",
      "[ColumnTransformer] .... (2 of 2) Processing word_2_vec, total= 2.6min\n",
      "[ColumnTransformer] . (1 of 2) Processing text_feat_ext, total= 3.8min\n",
      "[ColumnTransformer] ....... (1 of 1) Processing one_hot, total=   0.0s\n",
      "[ColumnTransformer] .. (2 of 2) Processing cat_feat_ext, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgxuser_layersvanguard/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "Multi_Naive_Bayes_Clf.fit(evaluation_metric = 'f1_weighted', hyperparameter_dict={'do_stem':(True,False),'alpha':(0,0.5,1),'fit_prior':(True, False)}, complete_search=True, n_cpus = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0      355.523128      7.491020        45.574739        3.455496   \n",
      "1      238.661791      6.522046        17.023449        0.721309   \n",
      "2      357.851137      6.784530        45.766461        3.426812   \n",
      "3      236.432991      2.175174        17.216562        1.049726   \n",
      "4      348.872430      7.446137        45.186421        2.856552   \n",
      "5      237.293326      1.504442        17.651727        1.317144   \n",
      "6      351.441696      9.621577        43.425756        5.091324   \n",
      "7      239.982294      3.789924        16.441844        0.741593   \n",
      "8      328.673169      3.299238        34.012448        2.780186   \n",
      "9      235.620549      8.850498        15.788453        1.625778   \n",
      "10     270.292642     26.381063        25.144721        2.137004   \n",
      "11     174.617510      3.257520         9.919657        0.655899   \n",
      "\n",
      "   param_clf__alpha param_clf__fit_prior  \\\n",
      "0                 0                 True   \n",
      "1                 0                 True   \n",
      "2                 0                False   \n",
      "3                 0                False   \n",
      "4               0.5                 True   \n",
      "5               0.5                 True   \n",
      "6               0.5                False   \n",
      "7               0.5                False   \n",
      "8                 1                 True   \n",
      "9                 1                 True   \n",
      "10                1                False   \n",
      "11                1                False   \n",
      "\n",
      "   param_feature_extraction_pipeline__text_feat_ext__stem__do_stem  \\\n",
      "0                                                True                \n",
      "1                                               False                \n",
      "2                                                True                \n",
      "3                                               False                \n",
      "4                                                True                \n",
      "5                                               False                \n",
      "6                                                True                \n",
      "7                                               False                \n",
      "8                                                True                \n",
      "9                                               False                \n",
      "10                                               True                \n",
      "11                                              False                \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0   {'clf__alpha': 0, 'clf__fit_prior': True, 'fea...           0.498042   \n",
      "1   {'clf__alpha': 0, 'clf__fit_prior': True, 'fea...           0.464965   \n",
      "2   {'clf__alpha': 0, 'clf__fit_prior': False, 'fe...           0.394383   \n",
      "3   {'clf__alpha': 0, 'clf__fit_prior': False, 'fe...           0.238632   \n",
      "4   {'clf__alpha': 0.5, 'clf__fit_prior': True, 'f...           0.498081   \n",
      "5   {'clf__alpha': 0.5, 'clf__fit_prior': True, 'f...           0.465818   \n",
      "6   {'clf__alpha': 0.5, 'clf__fit_prior': False, '...           0.402468   \n",
      "7   {'clf__alpha': 0.5, 'clf__fit_prior': False, '...           0.240641   \n",
      "8   {'clf__alpha': 1, 'clf__fit_prior': True, 'fea...           0.498113   \n",
      "9   {'clf__alpha': 1, 'clf__fit_prior': True, 'fea...           0.465774   \n",
      "10  {'clf__alpha': 1, 'clf__fit_prior': False, 'fe...           0.412861   \n",
      "11  {'clf__alpha': 1, 'clf__fit_prior': False, 'fe...           0.239820   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  \\\n",
      "0            0.472927           0.482741           0.480291   \n",
      "1            0.442517           0.454558           0.453950   \n",
      "2            0.368562           0.380812           0.370410   \n",
      "3            0.246832           0.251665           0.252729   \n",
      "4            0.472963           0.482733           0.480281   \n",
      "5            0.441295           0.456201           0.454317   \n",
      "6            0.367914           0.380629           0.370423   \n",
      "7            0.245925           0.254213           0.254081   \n",
      "8            0.472970           0.482745           0.480319   \n",
      "9            0.441508           0.456152           0.454272   \n",
      "10           0.367703           0.380072           0.370450   \n",
      "11           0.244946           0.254564           0.253871   \n",
      "\n",
      "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
      "0            0.475088         0.481818        0.008841                3  \n",
      "1            0.442686         0.451736        0.008424                6  \n",
      "2            0.361804         0.375195        0.011366                9  \n",
      "3            0.247917         0.247554        0.004979               12  \n",
      "4            0.475130         0.481838        0.008842                2  \n",
      "5            0.443737         0.452274        0.008905                5  \n",
      "6            0.360476         0.376383        0.014552                8  \n",
      "7            0.250448         0.249061        0.005185               10  \n",
      "8            0.475136         0.481857        0.008850                1  \n",
      "9            0.443693         0.452280        0.008841                4  \n",
      "10           0.360322         0.378283        0.018411                7  \n",
      "11           0.250838         0.248807        0.005632               11  \n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(Multi_Naive_Bayes_Clf.cross_val_result_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4818570910957413\n"
     ]
    }
   ],
   "source": [
    "print(Multi_Naive_Bayes_Clf.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random_Forest_Clf = ClfReddit(data_loc = reddit_data_loc, dim_word_vec=300, depth_post_col='depth',\n",
    "                 empty_post_str_indicator='[deleted]', label_col='labels', annotator_col='annotators',\n",
    "                 test_size=0.2, model = RandomForestClassifier(class_weight=\"balanced\", verbose=True, random_state=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_extraction_pipeline__text_feat_ext__stem__do_stem': (True, False), 'clf__max_depth': (2, 4, 8, 12)}\n",
      "Grid search for optimal hyper parameters\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done  34 out of  40 | elapsed:  5.5min remaining:   58.1s\n",
      "[Parallel(n_jobs=24)]: Done  40 out of  40 | elapsed:  6.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ColumnTransformer]  (1 of 2) Processing char_feat_extraction, total=   0.2s\n",
      "[ColumnTransformer] .... (2 of 2) Processing word_2_vec, total= 1.4min\n",
      "[ColumnTransformer] . (1 of 2) Processing text_feat_ext, total= 2.6min\n",
      "[ColumnTransformer] ....... (1 of 1) Processing one_hot, total=   0.0s\n",
      "[ColumnTransformer] .. (2 of 2) Processing cat_feat_ext, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgxuser_layersvanguard/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/dgxuser_layersvanguard/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "Random_Forest_Clf.fit(evaluation_metric = 'f1_weighted', hyperparameter_dict={'do_stem':(True,False),'max_depth':(2,4,8,12)}, complete_search=True, n_cpus = 24) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([247.24427543, 108.48075542, 242.97545943, 107.89756246,\n",
      "       236.17594523, 149.12637506, 183.51661625,  76.20044212]), 'std_fit_time': array([ 2.73896706,  3.44769513,  1.77394732,  2.84910215, 20.09258802,\n",
      "        5.13272591, 25.48898047,  2.14843026]), 'mean_score_time': array([24.55791125,  1.94768848, 25.44155664,  2.00675874, 23.09667039,\n",
      "        1.89552903, 15.30480242,  1.08372521]), 'std_score_time': array([1.99848677, 0.18262951, 2.27757435, 0.2057143 , 3.81162885,\n",
      "       0.25557936, 0.32235781, 0.02991108]), 'param_clf__max_depth': masked_array(data=[2, 2, 4, 4, 8, 8, 12, 12],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_feature_extraction_pipeline__text_feat_ext__stem__do_stem': masked_array(data=[True, False, True, False, True, False, True, False],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'clf__max_depth': 2, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': True}, {'clf__max_depth': 2, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': False}, {'clf__max_depth': 4, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': True}, {'clf__max_depth': 4, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': False}, {'clf__max_depth': 8, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': True}, {'clf__max_depth': 8, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': False}, {'clf__max_depth': 12, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': True}, {'clf__max_depth': 12, 'feature_extraction_pipeline__text_feat_ext__stem__do_stem': False}], 'split0_test_score': array([0.07540909, 0.03768125, 0.31108678, 0.10027239, 0.32813453,\n",
      "       0.31971111, 0.2663433 , 0.24248601]), 'split1_test_score': array([0.04848545, 0.06113945, 0.10441853, 0.12459617, 0.13196682,\n",
      "       0.15813734, 0.22429715, 0.33393543]), 'split2_test_score': array([0.07585372, 0.04104565, 0.11223714, 0.06752146, 0.17299291,\n",
      "       0.28342325, 0.29490663, 0.32263632]), 'split3_test_score': array([0.01025115, 0.01309056, 0.1865729 , 0.03922736, 0.23529537,\n",
      "       0.09032251, 0.41155927, 0.24017197]), 'split4_test_score': array([0.04980145, 0.02389248, 0.16142809, 0.10713097, 0.19850988,\n",
      "       0.10352119, 0.22947657, 0.14186246]), 'mean_test_score': array([0.05196133, 0.03537114, 0.17515188, 0.087751  , 0.213382  ,\n",
      "       0.19102996, 0.28531466, 0.25622318]), 'std_test_score': array([0.02398781, 0.01630792, 0.07451404, 0.03050784, 0.06653089,\n",
      "       0.09378432, 0.0681608 , 0.06924316]), 'rank_test_score': array([7, 8, 5, 6, 3, 4, 1, 2], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "print(Random_Forest_Clf.cross_val_result_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28531466151953105\n"
     ]
    }
   ],
   "source": [
    "print(Random_Forest_Clf.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_Clf= ClfReddit(data_loc = reddit_data_loc, do_kernel_transform=True, dim_word_vec = 300, depth_post_col='depth',\n",
    "                 empty_post_str_indicator='[deleted]', label_col='labels', annotator_col='annotators',\n",
    "                 test_size=0.2, self.model = SGDClassifier(loss='hinge', penalty='elasticnet', verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem': (True, False), 'feature_extraction_pipeline__kernel_transform__kernel': ('linear', 'rbf', 'poly'), 'feature_extraction_pipeline__kernel_transform__gamma': (0.001, 0.01, 0.1), 'clf__alpha': (0.01, 0.1, 0.5, 1, 2, 5), 'clf__l1_ratio': [0.0, 0.2, 0.4, 0.6000000000000001, 0.8, 1.0]}\n",
      "Randomized search for optimal hyper parameters\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   2 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=24)]: Done 152 tasks      | elapsed: 28.4min\n",
      "[Parallel(n_jobs=24)]: Done 250 out of 250 | elapsed: 51.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ColumnTransformer]  (1 of 2) Processing char_feat_extraction, total=   0.2s\n",
      "[ColumnTransformer] .... (2 of 2) Processing word_2_vec, total= 1.4min\n",
      "[ColumnTransformer] ....... (1 of 1) Processing one_hot, total=   0.0s\n",
      "[Pipeline] .......... (step 1 of 2) Processing feat_ext, total= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgxuser_layersvanguard/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .. (step 2 of 2) Processing kernel_transform, total=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.08, NNZs: 1000, Bias: -1.002669, T: 113223, Avg. loss: 0.101907\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.07, NNZs: 1000, Bias: -1.001232, T: 226446, Avg. loss: 0.101230\n",
      "Total training time: 1.30 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.07, NNZs: 1000, Bias: -1.000982, T: 339669, Avg. loss: 0.100956\n",
      "Total training time: 1.95 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.06, NNZs: 1000, Bias: -1.001029, T: 452892, Avg. loss: 0.100907\n",
      "Total training time: 2.60 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.06, NNZs: 1000, Bias: -1.000822, T: 566115, Avg. loss: 0.100895\n",
      "Total training time: 3.22 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.06, NNZs: 1000, Bias: -1.000672, T: 679338, Avg. loss: 0.100890\n",
      "Total training time: 3.86 seconds.\n",
      "Convergence after 6 epochs took 3.86 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.15, NNZs: 1000, Bias: -1.138741, T: 113223, Avg. loss: 0.027806\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.11, NNZs: 1000, Bias: -1.104184, T: 226446, Avg. loss: 0.026950\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.10, NNZs: 1000, Bias: -1.087578, T: 339669, Avg. loss: 0.026973\n",
      "Total training time: 1.81 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.09, NNZs: 1000, Bias: -1.076345, T: 452892, Avg. loss: 0.026867\n",
      "Total training time: 2.44 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.08, NNZs: 1000, Bias: -1.069596, T: 566115, Avg. loss: 0.026887\n",
      "Total training time: 3.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.07, NNZs: 1000, Bias: -1.063713, T: 679338, Avg. loss: 0.026860\n",
      "Total training time: 3.71 seconds.\n",
      "Convergence after 6 epochs took 3.71 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.92, NNZs: 1000, Bias: -0.811610, T: 113223, Avg. loss: 0.498235\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.93, NNZs: 1000, Bias: -0.808913, T: 226446, Avg. loss: 0.494232\n",
      "Total training time: 1.42 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.92, NNZs: 1000, Bias: -0.806392, T: 339669, Avg. loss: 0.493834\n",
      "Total training time: 2.15 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.92, NNZs: 1000, Bias: -0.808086, T: 452892, Avg. loss: 0.493921\n",
      "Total training time: 2.90 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.93, NNZs: 1000, Bias: -0.808415, T: 566115, Avg. loss: 0.493875\n",
      "Total training time: 3.63 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.92, NNZs: 1000, Bias: -0.807392, T: 679338, Avg. loss: 0.493864\n",
      "Total training time: 4.30 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.92, NNZs: 1000, Bias: -0.807242, T: 792561, Avg. loss: 0.493826\n",
      "Total training time: 5.04 seconds.\n",
      "Convergence after 7 epochs took 5.04 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.86, NNZs: 1000, Bias: -1.012650, T: 113223, Avg. loss: 0.182308\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.86, NNZs: 1000, Bias: -1.011603, T: 226446, Avg. loss: 0.180038\n",
      "Total training time: 1.15 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.85, NNZs: 1000, Bias: -1.011315, T: 339669, Avg. loss: 0.179792\n",
      "Total training time: 1.71 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.85, NNZs: 1000, Bias: -1.009318, T: 452892, Avg. loss: 0.179717\n",
      "Total training time: 2.29 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.85, NNZs: 1000, Bias: -1.010294, T: 566115, Avg. loss: 0.179673\n",
      "Total training time: 2.85 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.85, NNZs: 1000, Bias: -1.010045, T: 679338, Avg. loss: 0.179679\n",
      "Total training time: 3.40 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.85, NNZs: 1000, Bias: -1.013614, T: 792561, Avg. loss: 0.179653\n",
      "Total training time: 3.96 seconds.\n",
      "Convergence after 7 epochs took 3.96 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.03, NNZs: 1000, Bias: -1.002566, T: 113223, Avg. loss: 0.074535\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.02, NNZs: 1000, Bias: -1.000943, T: 226446, Avg. loss: 0.073797\n",
      "Total training time: 1.44 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.000247, T: 339669, Avg. loss: 0.073770\n",
      "Total training time: 2.13 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.000280, T: 452892, Avg. loss: 0.073757\n",
      "Total training time: 2.83 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.000277, T: 566115, Avg. loss: 0.073751\n",
      "Total training time: 3.45 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.000273, T: 679338, Avg. loss: 0.073748\n",
      "Total training time: 4.05 seconds.\n",
      "Convergence after 6 epochs took 4.05 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.08, NNZs: 1000, Bias: -1.002964, T: 113223, Avg. loss: 0.395946\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.05, NNZs: 1000, Bias: -1.000842, T: 226446, Avg. loss: 0.393083\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.04, NNZs: 1000, Bias: -0.999703, T: 339669, Avg. loss: 0.392673\n",
      "Total training time: 2.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.03, NNZs: 1000, Bias: -1.000443, T: 452892, Avg. loss: 0.392607\n",
      "Total training time: 2.90 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.03, NNZs: 1000, Bias: -0.999999, T: 566115, Avg. loss: 0.392573\n",
      "Total training time: 3.57 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.02, NNZs: 1000, Bias: -0.999709, T: 679338, Avg. loss: 0.392550\n",
      "Total training time: 4.19 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.02, NNZs: 1000, Bias: -1.000371, T: 792561, Avg. loss: 0.392555\n",
      "Total training time: 4.93 seconds.\n",
      "Convergence after 7 epochs took 4.93 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.03, NNZs: 1000, Bias: -1.000345, T: 113223, Avg. loss: 0.064213\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.02, NNZs: 1000, Bias: -1.000435, T: 226446, Avg. loss: 0.063150\n",
      "Total training time: 1.27 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.02, NNZs: 1000, Bias: -1.000336, T: 339669, Avg. loss: 0.063137\n",
      "Total training time: 1.94 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.000060, T: 452892, Avg. loss: 0.063202\n",
      "Total training time: 2.59 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.000065, T: 566115, Avg. loss: 0.063102\n",
      "Total training time: 3.21 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.000212, T: 679338, Avg. loss: 0.063120\n",
      "Total training time: 3.87 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.01, NNZs: 1000, Bias: -0.999941, T: 792561, Avg. loss: 0.063098\n",
      "Total training time: 4.43 seconds.\n",
      "Convergence after 7 epochs took 4.43 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.03, NNZs: 1000, Bias: -1.004116, T: 113223, Avg. loss: 0.048095\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.02, NNZs: 1000, Bias: -1.002001, T: 226446, Avg. loss: 0.047658\n",
      "Total training time: 1.29 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.02, NNZs: 1000, Bias: -1.001173, T: 339669, Avg. loss: 0.047665\n",
      "Total training time: 1.83 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.02, NNZs: 1000, Bias: -1.000635, T: 452892, Avg. loss: 0.047616\n",
      "Total training time: 2.59 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.000420, T: 566115, Avg. loss: 0.047610\n",
      "Total training time: 3.18 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.000110, T: 679338, Avg. loss: 0.047620\n",
      "Total training time: 3.80 seconds.\n",
      "Convergence after 6 epochs took 3.80 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.24, NNZs: 1000, Bias: -1.001200, T: 113223, Avg. loss: 0.056385\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.02, NNZs: 1000, Bias: -1.000636, T: 226446, Avg. loss: 0.055929\n",
      "Total training time: 1.21 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.02, NNZs: 1000, Bias: -1.000287, T: 339669, Avg. loss: 0.055646\n",
      "Total training time: 1.85 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.000079, T: 452892, Avg. loss: 0.055632\n",
      "Total training time: 2.44 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.000123, T: 566115, Avg. loss: 0.055644\n",
      "Total training time: 3.14 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.01, NNZs: 1000, Bias: -1.000141, T: 679338, Avg. loss: 0.055614\n",
      "Total training time: 3.70 seconds.\n",
      "Convergence after 6 epochs took 3.70 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.79, NNZs: 1000, Bias: -0.841356, T: 113223, Avg. loss: 0.232632\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.80, NNZs: 1000, Bias: -0.839526, T: 226446, Avg. loss: 0.229608\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.80, NNZs: 1000, Bias: -0.836181, T: 339669, Avg. loss: 0.229393\n",
      "Total training time: 1.77 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.81, NNZs: 1000, Bias: -0.834118, T: 452892, Avg. loss: 0.229206\n",
      "Total training time: 2.37 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.81, NNZs: 1000, Bias: -0.837587, T: 566115, Avg. loss: 0.229064\n",
      "Total training time: 3.03 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.81, NNZs: 1000, Bias: -0.834465, T: 679338, Avg. loss: 0.229143\n",
      "Total training time: 3.70 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.81, NNZs: 1000, Bias: -0.833905, T: 792561, Avg. loss: 0.229080\n",
      "Total training time: 4.32 seconds.\n",
      "Convergence after 7 epochs took 4.32 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   41.8s finished\n"
     ]
    }
   ],
   "source": [
    "SVM_Clf.fit(evaluation_metric = 'f1_weighted', hyperparameter_dict={'do_stem':(True,False),\n",
    "                    'kernel': ('linear','rbf', 'poly'), \n",
    "                   'gamma':(0.001, 0.01, 0.1), \n",
    "                   'alpha': (0.01, 0.1, 0.5, 1, 2, 5), \n",
    "                   'l1_ratio': list(np.arange(0, 1.1, 0.2))}, complete_search=False,  n_cpus = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4811716908938739\n"
     ]
    }
   ],
   "source": [
    "print(SVM_Clf.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0      286.859710      5.608673        31.243010        2.126599   \n",
      "1      158.597763      5.966100         3.926505        0.245303   \n",
      "2      167.962363     13.610342         4.012682        0.301760   \n",
      "3      290.613299      7.055830        30.544755        2.924448   \n",
      "4      285.465329      6.638721        30.459836        2.703715   \n",
      "5      184.497311      4.456043         4.615420        0.276464   \n",
      "6      296.568942     18.297247        33.671202        2.496465   \n",
      "7      292.571996      9.394170        32.214281        3.046271   \n",
      "8      183.416566      3.059091         3.635013        0.620759   \n",
      "9      190.588649      4.366860         3.633689        0.346186   \n",
      "10     347.834113    156.404173         4.564969        0.776781   \n",
      "11     185.587637      6.517457         3.707207        0.256470   \n",
      "12     279.847340     10.857185        33.253712        2.068510   \n",
      "13     240.147722     14.915691         5.134680        1.113776   \n",
      "14     277.103156     11.530947        26.915853        2.080076   \n",
      "15     182.020218      2.887790         2.847088        0.855989   \n",
      "16     240.871489     23.648798         5.106747        1.066681   \n",
      "17     241.633571      7.084452        24.646167        1.730862   \n",
      "18     247.724866     21.104948        24.322175        0.651217   \n",
      "19     169.805367     27.424528         5.474542        1.406894   \n",
      "20     233.103858      6.225968        27.224461        4.182185   \n",
      "21     254.895187     16.009244        27.198257        2.618621   \n",
      "22     165.269880      3.222468         4.540345        0.781749   \n",
      "23     168.179760      4.662668         5.272289        1.477170   \n",
      "24     288.505425     38.978278        24.756590        1.121985   \n",
      "25     207.449455     35.897930         6.395737        1.137503   \n",
      "26     406.117907    284.435281         5.789528        1.489058   \n",
      "27     179.760463     11.100319         4.044320        0.542464   \n",
      "28     242.089197     10.934388        25.690596        3.220114   \n",
      "29     146.957858      5.230332         4.919925        1.481200   \n",
      "30    1047.141659    515.154589         3.443164        0.669543   \n",
      "31     221.512569      6.958496        24.792872        2.703999   \n",
      "32     173.245136     43.211107         4.509611        1.233381   \n",
      "33     223.327886      7.613502        24.105795        1.472562   \n",
      "34     153.299486      3.987900         4.944599        0.441941   \n",
      "35     149.766764      2.686807         4.532681        1.105691   \n",
      "36     148.658195      5.704371         3.247334        1.327594   \n",
      "37     165.514451     12.864874         4.611868        1.381771   \n",
      "38     149.863583      2.915550         3.575664        1.044165   \n",
      "39     228.819334      3.512914        20.861475        2.116071   \n",
      "40     141.734704      4.378954         2.051587        0.189691   \n",
      "41     215.463564      7.382965        26.649144        2.109032   \n",
      "42     228.869778      6.488099        26.902851        2.211484   \n",
      "43     220.998189      6.433289        22.419626        2.383305   \n",
      "44     150.756243      2.820749         3.014817        1.134963   \n",
      "45     158.248226     15.755691         4.525362        1.006166   \n",
      "46     282.277400     44.727112        20.546035        3.176858   \n",
      "47     239.102477     13.138359        20.342851        1.008185   \n",
      "48     223.465674      6.060682        17.710060        1.429486   \n",
      "49     148.437063      2.723193         4.033428        1.552512   \n",
      "\n",
      "   param_feature_extraction_pipeline__kernel_transform__kernel  \\\n",
      "0                                                 rbf            \n",
      "1                                                 rbf            \n",
      "2                                                 rbf            \n",
      "3                                              linear            \n",
      "4                                              linear            \n",
      "5                                                poly            \n",
      "6                                                 rbf            \n",
      "7                                                poly            \n",
      "8                                              linear            \n",
      "9                                                 rbf            \n",
      "10                                               poly            \n",
      "11                                                rbf            \n",
      "12                                                rbf            \n",
      "13                                               poly            \n",
      "14                                                rbf            \n",
      "15                                             linear            \n",
      "16                                               poly            \n",
      "17                                                rbf            \n",
      "18                                             linear            \n",
      "19                                                rbf            \n",
      "20                                             linear            \n",
      "21                                                rbf            \n",
      "22                                             linear            \n",
      "23                                             linear            \n",
      "24                                               poly            \n",
      "25                                               poly            \n",
      "26                                               poly            \n",
      "27                                               poly            \n",
      "28                                               poly            \n",
      "29                                                rbf            \n",
      "30                                               poly            \n",
      "31                                               poly            \n",
      "32                                             linear            \n",
      "33                                             linear            \n",
      "34                                             linear            \n",
      "35                                                rbf            \n",
      "36                                             linear            \n",
      "37                                                rbf            \n",
      "38                                             linear            \n",
      "39                                             linear            \n",
      "40                                             linear            \n",
      "41                                                rbf            \n",
      "42                                               poly            \n",
      "43                                             linear            \n",
      "44                                             linear            \n",
      "45                                               poly            \n",
      "46                                               poly            \n",
      "47                                                rbf            \n",
      "48                                             linear            \n",
      "49                                                rbf            \n",
      "\n",
      "   param_feature_extraction_pipeline__kernel_transform__gamma  \\\n",
      "0                                                 0.1           \n",
      "1                                                0.01           \n",
      "2                                                 0.1           \n",
      "3                                                 0.1           \n",
      "4                                                 0.1           \n",
      "5                                               0.001           \n",
      "6                                               0.001           \n",
      "7                                                0.01           \n",
      "8                                                0.01           \n",
      "9                                                0.01           \n",
      "10                                                0.1           \n",
      "11                                                0.1           \n",
      "12                                               0.01           \n",
      "13                                                0.1           \n",
      "14                                                0.1           \n",
      "15                                                0.1           \n",
      "16                                                0.1           \n",
      "17                                               0.01           \n",
      "18                                              0.001           \n",
      "19                                                0.1           \n",
      "20                                              0.001           \n",
      "21                                               0.01           \n",
      "22                                              0.001           \n",
      "23                                               0.01           \n",
      "24                                                0.1           \n",
      "25                                               0.01           \n",
      "26                                               0.01           \n",
      "27                                               0.01           \n",
      "28                                               0.01           \n",
      "29                                              0.001           \n",
      "30                                                0.1           \n",
      "31                                               0.01           \n",
      "32                                               0.01           \n",
      "33                                                0.1           \n",
      "34                                              0.001           \n",
      "35                                                0.1           \n",
      "36                                               0.01           \n",
      "37                                               0.01           \n",
      "38                                              0.001           \n",
      "39                                               0.01           \n",
      "40                                              0.001           \n",
      "41                                               0.01           \n",
      "42                                              0.001           \n",
      "43                                                0.1           \n",
      "44                                              0.001           \n",
      "45                                               0.01           \n",
      "46                                                0.1           \n",
      "47                                                0.1           \n",
      "48                                              0.001           \n",
      "49                                               0.01           \n",
      "\n",
      "   param_feature_extraction_pipeline__feat_ext__text_feat_ext__stem__do_stem  \\\n",
      "0                                                True                          \n",
      "1                                               False                          \n",
      "2                                               False                          \n",
      "3                                                True                          \n",
      "4                                                True                          \n",
      "5                                               False                          \n",
      "6                                                True                          \n",
      "7                                                True                          \n",
      "8                                               False                          \n",
      "9                                               False                          \n",
      "10                                              False                          \n",
      "11                                              False                          \n",
      "12                                               True                          \n",
      "13                                              False                          \n",
      "14                                               True                          \n",
      "15                                              False                          \n",
      "16                                              False                          \n",
      "17                                               True                          \n",
      "18                                               True                          \n",
      "19                                              False                          \n",
      "20                                               True                          \n",
      "21                                               True                          \n",
      "22                                              False                          \n",
      "23                                              False                          \n",
      "24                                               True                          \n",
      "25                                              False                          \n",
      "26                                              False                          \n",
      "27                                              False                          \n",
      "28                                               True                          \n",
      "29                                              False                          \n",
      "30                                              False                          \n",
      "31                                               True                          \n",
      "32                                              False                          \n",
      "33                                               True                          \n",
      "34                                              False                          \n",
      "35                                              False                          \n",
      "36                                              False                          \n",
      "37                                              False                          \n",
      "38                                              False                          \n",
      "39                                               True                          \n",
      "40                                              False                          \n",
      "41                                               True                          \n",
      "42                                               True                          \n",
      "43                                               True                          \n",
      "44                                              False                          \n",
      "45                                              False                          \n",
      "46                                               True                          \n",
      "47                                               True                          \n",
      "48                                               True                          \n",
      "49                                              False                          \n",
      "\n",
      "   param_clf__l1_ratio param_clf__alpha  \\\n",
      "0                  0.2              0.1   \n",
      "1                  0.4              0.1   \n",
      "2                    1                2   \n",
      "3                    1                1   \n",
      "4                  0.2                1   \n",
      "5                  0.8              0.5   \n",
      "6                  0.4                5   \n",
      "7                    0              0.5   \n",
      "8                  0.4              0.1   \n",
      "9                  0.4                1   \n",
      "10                 0.6                5   \n",
      "11                   0                5   \n",
      "12                   1              0.5   \n",
      "13                   0                2   \n",
      "14                   1             0.01   \n",
      "15                   1              0.1   \n",
      "16                   0              0.1   \n",
      "17                   0                1   \n",
      "18                 0.8                2   \n",
      "19                 0.2                5   \n",
      "20                 0.4                5   \n",
      "21                   0                2   \n",
      "22                   0              0.5   \n",
      "23                 0.4              0.5   \n",
      "24                 0.4                2   \n",
      "25                 0.2              0.1   \n",
      "26                   1              0.5   \n",
      "27                 0.2                2   \n",
      "28                   0                5   \n",
      "29                 0.6                1   \n",
      "30                 0.8              0.1   \n",
      "31                 0.2              0.5   \n",
      "32                 0.8                5   \n",
      "33                 0.2             0.01   \n",
      "34                   0              0.1   \n",
      "35                   1             0.01   \n",
      "36                   1                1   \n",
      "37                 0.2                2   \n",
      "38                 0.6                1   \n",
      "39                   0             0.01   \n",
      "40                 0.2                1   \n",
      "41                 0.2                2   \n",
      "42                   0              0.5   \n",
      "43                 0.6                1   \n",
      "44                 0.4              0.1   \n",
      "45                 0.4                2   \n",
      "46                 0.4              0.5   \n",
      "47                 0.8                2   \n",
      "48                 0.2                1   \n",
      "49                 0.2             0.01   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0   {'feature_extraction_pipeline__kernel_transfor...           0.041239   \n",
      "1   {'feature_extraction_pipeline__kernel_transfor...           0.041239   \n",
      "2   {'feature_extraction_pipeline__kernel_transfor...           0.015938   \n",
      "3   {'feature_extraction_pipeline__kernel_transfor...           0.064374   \n",
      "4   {'feature_extraction_pipeline__kernel_transfor...           0.041239   \n",
      "5   {'feature_extraction_pipeline__kernel_transfor...           0.064374   \n",
      "6   {'feature_extraction_pipeline__kernel_transfor...           0.201972   \n",
      "7   {'feature_extraction_pipeline__kernel_transfor...           0.470946   \n",
      "8   {'feature_extraction_pipeline__kernel_transfor...           0.001106   \n",
      "9   {'feature_extraction_pipeline__kernel_transfor...           0.041239   \n",
      "10  {'feature_extraction_pipeline__kernel_transfor...           0.004849   \n",
      "11  {'feature_extraction_pipeline__kernel_transfor...           0.493010   \n",
      "12  {'feature_extraction_pipeline__kernel_transfor...           0.041239   \n",
      "13  {'feature_extraction_pipeline__kernel_transfor...           0.471319   \n",
      "14  {'feature_extraction_pipeline__kernel_transfor...           0.064374   \n",
      "15  {'feature_extraction_pipeline__kernel_transfor...           0.201972   \n",
      "16  {'feature_extraction_pipeline__kernel_transfor...           0.314165   \n",
      "17  {'feature_extraction_pipeline__kernel_transfor...           0.393954   \n",
      "18  {'feature_extraction_pipeline__kernel_transfor...           0.064374   \n",
      "19  {'feature_extraction_pipeline__kernel_transfor...           0.064374   \n",
      "20  {'feature_extraction_pipeline__kernel_transfor...           0.201972   \n",
      "21  {'feature_extraction_pipeline__kernel_transfor...           0.358754   \n",
      "22  {'feature_extraction_pipeline__kernel_transfor...           0.458694   \n",
      "23  {'feature_extraction_pipeline__kernel_transfor...           0.064374   \n",
      "24  {'feature_extraction_pipeline__kernel_transfor...           0.201972   \n",
      "25  {'feature_extraction_pipeline__kernel_transfor...           0.214379   \n",
      "26  {'feature_extraction_pipeline__kernel_transfor...           0.190169   \n",
      "27  {'feature_extraction_pipeline__kernel_transfor...           0.219979   \n",
      "28  {'feature_extraction_pipeline__kernel_transfor...           0.202174   \n",
      "29  {'feature_extraction_pipeline__kernel_transfor...           0.201972   \n",
      "30  {'feature_extraction_pipeline__kernel_transfor...           0.158271   \n",
      "31  {'feature_extraction_pipeline__kernel_transfor...           0.201972   \n",
      "32  {'feature_extraction_pipeline__kernel_transfor...           0.064374   \n",
      "33  {'feature_extraction_pipeline__kernel_transfor...           0.384285   \n",
      "34  {'feature_extraction_pipeline__kernel_transfor...           0.438911   \n",
      "35  {'feature_extraction_pipeline__kernel_transfor...           0.015938   \n",
      "36  {'feature_extraction_pipeline__kernel_transfor...           0.064374   \n",
      "37  {'feature_extraction_pipeline__kernel_transfor...           0.201972   \n",
      "38  {'feature_extraction_pipeline__kernel_transfor...           0.201972   \n",
      "39  {'feature_extraction_pipeline__kernel_transfor...           0.480996   \n",
      "40  {'feature_extraction_pipeline__kernel_transfor...           0.201972   \n",
      "41  {'feature_extraction_pipeline__kernel_transfor...           0.041239   \n",
      "42  {'feature_extraction_pipeline__kernel_transfor...           0.431532   \n",
      "43  {'feature_extraction_pipeline__kernel_transfor...           0.201972   \n",
      "44  {'feature_extraction_pipeline__kernel_transfor...           0.064374   \n",
      "45  {'feature_extraction_pipeline__kernel_transfor...           0.201972   \n",
      "46  {'feature_extraction_pipeline__kernel_transfor...           0.201972   \n",
      "47  {'feature_extraction_pipeline__kernel_transfor...           0.064374   \n",
      "48  {'feature_extraction_pipeline__kernel_transfor...           0.001932   \n",
      "49  {'feature_extraction_pipeline__kernel_transfor...           0.289122   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  \\\n",
      "0            0.201987           0.041253           0.041234   \n",
      "1            0.002622           0.002623           0.064373   \n",
      "2            0.201987           0.041253           0.202049   \n",
      "3            0.041242           0.202034           0.001928   \n",
      "4            0.201987           0.202034           0.041234   \n",
      "5            0.201987           0.202034           0.202049   \n",
      "6            0.201987           0.015943           0.202049   \n",
      "7            0.458779           0.413879           0.378069   \n",
      "8            0.204294           0.202034           0.309177   \n",
      "9            0.064379           0.041253           0.202049   \n",
      "10           0.259190           0.189202           0.191624   \n",
      "11           0.417690           0.382634           0.202049   \n",
      "12           0.002622           0.041253           0.015944   \n",
      "13           0.295847           0.285206           0.181364   \n",
      "14           0.015939           0.041253           0.202049   \n",
      "15           0.201987           0.004843           0.064373   \n",
      "16           0.317025           0.304995           0.266381   \n",
      "17           0.475437           0.457726           0.423372   \n",
      "18           0.201987           0.202034           0.064373   \n",
      "19           0.201987           0.202034           0.015944   \n",
      "20           0.015939           0.202034           0.202049   \n",
      "21           0.411225           0.397295           0.373407   \n",
      "22           0.420011           0.425611           0.407247   \n",
      "23           0.064379           0.015943           0.202049   \n",
      "24           0.206487           0.207972           0.036281   \n",
      "25           0.165744           0.191749           0.213614   \n",
      "26           0.138480           0.182669           0.170340   \n",
      "27           0.218040           0.196725           0.191529   \n",
      "28           0.201987           0.461009           0.202049   \n",
      "29           0.201987           0.202034           0.015944   \n",
      "30           0.246513           0.330262           0.311951   \n",
      "31           0.201987           0.064368           0.202049   \n",
      "32           0.201987           0.202034           0.202049   \n",
      "33           0.449046           0.377011           0.387867   \n",
      "34           0.413041           0.417766           0.441296   \n",
      "35           0.064379           0.064368           0.202049   \n",
      "36           0.004850           0.202034           0.064373   \n",
      "37           0.201987           0.202034           0.202049   \n",
      "38           0.201987           0.202034           0.004843   \n",
      "39           0.504861           0.478085           0.486794   \n",
      "40           0.201987           0.041253           0.041234   \n",
      "41           0.201987           0.041253           0.064373   \n",
      "42           0.347492           0.381382           0.375313   \n",
      "43           0.064379           0.064368           0.015944   \n",
      "44           0.297705           0.245334           0.001928   \n",
      "45           0.006880           0.108484           0.174602   \n",
      "46           0.243384           0.130655           0.247365   \n",
      "47           0.201987           0.064368           0.202049   \n",
      "48           0.201987           0.064368           0.202049   \n",
      "49           0.173796           0.041321           0.325308   \n",
      "\n",
      "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
      "0            0.015947         0.068336        0.067544               46  \n",
      "1            0.064383         0.035046        0.027791               49  \n",
      "2            0.064383         0.105122        0.080588               40  \n",
      "3            0.015947         0.065106        0.071730               48  \n",
      "4            0.202080         0.137712        0.078775               31  \n",
      "5            0.064383         0.146966        0.067432               29  \n",
      "6            0.202080         0.164807        0.074431               25  \n",
      "7            0.418892         0.428116        0.033369                3  \n",
      "8            0.209674         0.185249        0.100510               18  \n",
      "9            0.202080         0.110193        0.075480               37  \n",
      "10           0.173437         0.163657        0.084713               27  \n",
      "11           0.389553         0.376993        0.095824                9  \n",
      "12           0.064383         0.033087        0.021608               50  \n",
      "13           0.283491         0.303453        0.093677               11  \n",
      "14           0.002617         0.065245        0.071615               47  \n",
      "15           0.004844         0.095613        0.089536               41  \n",
      "16           0.322369         0.304988        0.020108               10  \n",
      "17           0.385152         0.427129        0.035062                4  \n",
      "18           0.001502         0.106858        0.081015               39  \n",
      "19           0.202080         0.137282        0.080767               33  \n",
      "20           0.202080         0.164811        0.074441               24  \n",
      "21           0.392423         0.386620        0.018462                8  \n",
      "22           0.429121         0.428138        0.016992                2  \n",
      "23           0.041241         0.077596        0.064745               42  \n",
      "24           0.242995         0.179143        0.072917               19  \n",
      "25           0.108162         0.178732        0.039510               20  \n",
      "26           0.196477         0.175626        0.020512               21  \n",
      "27           0.204526         0.206161        0.011295               15  \n",
      "28           0.202175         0.253878        0.103564               12  \n",
      "29           0.202080         0.164806        0.074428               26  \n",
      "30           0.198524         0.249102        0.065349               13  \n",
      "31           0.202080         0.174492        0.055061               23  \n",
      "32           0.202080         0.174501        0.055069               22  \n",
      "33           0.383060         0.396255        0.026628                7  \n",
      "34           0.411160         0.424435        0.012995                5  \n",
      "35           0.041241         0.077592        0.064747               43  \n",
      "36           0.202080         0.107536        0.080173               38  \n",
      "37           0.202080         0.202024        0.000040               17  \n",
      "38           0.064383         0.135051        0.084134               34  \n",
      "39           0.455117         0.481172        0.016007                1  \n",
      "40           0.202080         0.137708        0.078760               32  \n",
      "41           0.004844         0.070743        0.068337               45  \n",
      "42           0.471448         0.401431        0.044275                6  \n",
      "43           0.041241         0.077586        0.064720               44  \n",
      "44           0.098986         0.141669        0.111731               30  \n",
      "45           0.171372         0.132660        0.069956               36  \n",
      "46           0.217944         0.208264        0.042234               14  \n",
      "47           0.202080         0.146968        0.067444               28  \n",
      "48           0.202080         0.134478        0.085064               35  \n",
      "49           0.184119         0.202735        0.099724               16  \n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(SVM_Clf.cross_val_result_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see from performance of three models, \n",
    "\n",
    "* Multi Naive Bayes using no word embeddings outperforms every other model. \n",
    "* Also the inferenece time require for the same model is very low as compared to most of the models. Only SVM with no   word embeddings has lower inference time. \n",
    "\n",
    "Considering both the evaluation metric and inference time and simplicity of the model we decide to go with Multi Naive Bayes model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Multi_Naive_Bayes_Clf_no_embedding.save_model(\"multi_naive_bayes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
